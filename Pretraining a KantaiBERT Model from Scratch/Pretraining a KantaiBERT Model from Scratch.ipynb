{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "357097e8-3c38-4a14-a101-0cc416e41be6",
   "metadata": {
    "id": "357097e8-3c38-4a14-a101-0cc416e41be6",
    "tags": []
   },
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84b6f4f4-7878-4a7c-ac23-d2737798773d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "84b6f4f4-7878-4a7c-ac23-d2737798773d",
    "outputId": "81b783ec-effd-4df5-c883-88fa1f168cad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 10.7M  100 10.7M    0     0  20.6M      0 --:--:-- --:--:-- --:--:-- 20.6M\n"
     ]
    }
   ],
   "source": [
    "!curl -L https://raw.githubusercontent.com/PacktPublishing/Transformers-for-Natural-Language-Processing/master/Chapter03/kant.txt --output \"kant.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "gpF4l_ruF_9a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gpF4l_ruF_9a",
    "outputId": "922cca0b-f5dd-4dcb-e960-e2f53dab068d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
      "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m101.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mtokenizers                    0.13.3\n",
      "transformers                  4.29.2\n"
     ]
    }
   ],
   "source": [
    "# Install `transformers` from master\n",
    "!pip install transformers\n",
    "!pip list | grep -E 'transformers|tokenizers'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec6c69b-a465-4aa9-ba68-4909756afa06",
   "metadata": {
    "id": "2ec6c69b-a465-4aa9-ba68-4909756afa06",
    "tags": []
   },
   "source": [
    "## Training a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab08866f-f7b0-4457-95e8-c3adbbaf2c31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ab08866f-f7b0-4457-95e8-c3adbbaf2c31",
    "outputId": "1ac86230-7719-4283-e3e8-1d69f70b55ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.46 s, sys: 181 ms, total: 7.64 s\n",
      "Wall time: 4.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pathlib import Path\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "paths = [str(x) for x in Path('.').glob(\"**/*.txt\")]\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "tokenizer.train(files = paths, vocab_size = 52000, min_frequency = 2, special_tokens = [\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57a747b9-6e7a-47a3-aaf4-3e92d64c706f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "57a747b9-6e7a-47a3-aaf4-3e92d64c706f",
    "outputId": "e10365ee-fbae-4f86-e6a9-9ce01599e15b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['KantaiBERT/vocab.json', 'KantaiBERT/merges.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "token_dir = 'KantaiBERT'\n",
    "if not os.path.exists(token_dir):\n",
    "    os.makedirs(token_dir)\n",
    "tokenizer.save_model('KantaiBERT')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29df06a-6eae-4be0-a08d-efe59a3daa04",
   "metadata": {
    "id": "d29df06a-6eae-4be0-a08d-efe59a3daa04",
    "tags": []
   },
   "source": [
    "## Loading the trained tokenizer files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48127bf4-7209-40c7-aea2-c6e46b27530e",
   "metadata": {
    "id": "48127bf4-7209-40c7-aea2-c6e46b27530e"
   },
   "outputs": [],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    './KantaiBERT/vocab.json', \n",
    "    './KantaiBERT/merges.txt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01fb6d9b-d642-4b9c-a79e-56559f18cc58",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "01fb6d9b-d642-4b9c-a79e-56559f18cc58",
    "outputId": "a6da28eb-38e6-459e-9937-f0e325c12cef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'ĠCritique', 'Ġof', 'ĠPure', 'ĠReason']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"The Critique of Pure Reason\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "707ebb15-6bae-4b42-9881-30786e847c99",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "707ebb15-6bae-4b42-9881-30786e847c99",
    "outputId": "85598bb6-40aa-48ed-f1ca-83f726f74bbc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=6, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"The Critique of Pure Reason.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b01abe1c-58c5-428e-a33d-cbb92f3d4283",
   "metadata": {
    "id": "b01abe1c-58c5-428e-a33d-cbb92f3d4283"
   },
   "outputs": [],
   "source": [
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    ")\n",
    "\n",
    "tokenizer.enable_truncation(max_length= 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a69271e-55f1-4a49-a029-aea827dfe195",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0a69271e-55f1-4a49-a029-aea827dfe195",
    "outputId": "7e7a69b2-2bb3-4eb1-db49-9ba871e51898"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=8, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"The Critique of Pure Reason.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d4ba683-0b08-4c5d-a639-c13e54b34066",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3d4ba683-0b08-4c5d-a639-c13e54b34066",
    "outputId": "359552ca-bcc5-46f3-fa47-14e1f0b0603e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s>', 'The', 'ĠCritique', 'Ġof', 'ĠPure', 'ĠReason', '</s>']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"The Critique of Pure Reason\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "272c1417-f097-435b-bd1a-6faa1a2978d7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "272c1417-f097-435b-bd1a-6faa1a2978d7",
    "outputId": "f0a619e2-724f-4bc8-db9d-ec9425d5e2aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 29 15:22:49 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   47C    P8    13W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89ac776f-20d6-466c-af36-3601e16abbf7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "89ac776f-20d6-466c-af36-3601e16abbf7",
    "outputId": "301debd0-a47f-40e4-bd3c-55e516c92296"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b2cdef-49bc-4a6d-96ac-417094dc99af",
   "metadata": {
    "id": "61b2cdef-49bc-4a6d-96ac-417094dc99af",
    "tags": []
   },
   "source": [
    "## Defining the configuration of the model\n",
    "We will be pretraining a RoBERTa-type transformer model using the same number\n",
    "of layers and heads as a DistilBERT transformer. The model will have a vocabulary\n",
    "size set to 52,000, 12 attention heads, and 6 layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27528b44-0ee1-4499-b407-84a86e184105",
   "metadata": {
    "id": "27528b44-0ee1-4499-b407-84a86e184105"
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size = 52000,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers = 6,\n",
    "    type_vocab_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f9bf5f-da8f-49d2-b8b8-ff8d73e5419b",
   "metadata": {
    "id": "18f9bf5f-da8f-49d2-b8b8-ff8d73e5419b",
    "tags": []
   },
   "source": [
    "## Reloading the tokenizer in transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "703ad588-b05e-4456-bf73-36fe365b66ef",
   "metadata": {
    "id": "703ad588-b05e-4456-bf73-36fe365b66ef"
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"./KantaiBERT\", max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aa4a2e-ed0f-47d9-b1f5-48cc9d091989",
   "metadata": {
    "id": "a1aa4a2e-ed0f-47d9-b1f5-48cc9d091989",
    "tags": []
   },
   "source": [
    "## Initializing a model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "509b8717-877a-4630-b445-6c43ac1a8c84",
   "metadata": {
    "id": "509b8717-877a-4630-b445-6c43ac1a8c84"
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "\n",
    "model = RobertaForMaskedLM(config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76036050-ac21-473c-b149-f7d6582b5f80",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "76036050-ac21-473c-b149-f7d6582b5f80",
    "outputId": "b8968ea9-1a6a-415c-8900-501dd0bee53e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForMaskedLM(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(52000, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=52000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc78b924-a58b-44ed-86b8-1e82f10cdfd2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fc78b924-a58b-44ed-86b8-1e82f10cdfd2",
    "outputId": "e78677c7-d66e-48f0-c496-5ff3b03f9436"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83504416"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c2cedcd-dceb-4024-8454-29a6ca5546ae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3c2cedcd-dceb-4024-8454-29a6ca5546ae",
    "outputId": "c3b590e7-beef-4e99-b25d-bb38ddd2c17f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106\n"
     ]
    }
   ],
   "source": [
    "LP = list(model.parameters())\n",
    "lp = len(LP)\n",
    "print(lp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb776153-72bf-4ab7-9861-98efafdbf45b",
   "metadata": {
    "id": "bb776153-72bf-4ab7-9861-98efafdbf45b"
   },
   "source": [
    "## Building the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95175f4c-bd54-4096-9a89-be09c07b31a4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95175f4c-bd54-4096-9a89-be09c07b31a4",
    "outputId": "2f81a6ff-c5ed-4a77-8799-25616f5af4ce"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:119: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.1 s, sys: 552 ms, total: 28.6 s\n",
      "Wall time: 34.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from transformers import LineByLineTextDataset\n",
    "\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer = tokenizer, \n",
    "    file_path= \"./kant.txt\",\n",
    "    block_size = 128,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e01ad97-09c5-419d-a241-22ab787ad6f6",
   "metadata": {
    "id": "3e01ad97-09c5-419d-a241-22ab787ad6f6"
   },
   "source": [
    "## Defining a data collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f4a4ba5-e547-4cdf-8105-bb687bbd624b",
   "metadata": {
    "id": "6f4a4ba5-e547-4cdf-8105-bb687bbd624b"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer= tokenizer, mlm = True, mlm_probability= 0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "xg9QIpbfGiKw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xg9QIpbfGiKw",
    "outputId": "2e460787-69da-473f-fa27-3a14ba9f6db8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.19.0-py3-none-any.whl (219 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.1/219.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (4.5.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (3.25.2)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (16.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: accelerate\n",
      "Successfully installed accelerate-0.19.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185663d1-91c6-4c28-ac22-d2325e3d908f",
   "metadata": {
    "id": "185663d1-91c6-4c28-ac22-d2325e3d908f"
   },
   "source": [
    "## Initializing the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ea39999-df07-4e53-9565-7726b9799bb3",
   "metadata": {
    "id": "1ea39999-df07-4e53-9565-7726b9799bb3"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./KantaiBERT\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=64,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0986da6-e0a8-46f9-b6b6-b2b8b77ed410",
   "metadata": {
    "id": "d0986da6-e0a8-46f9-b6b6-b2b8b77ed410"
   },
   "source": [
    "## Pretraining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d69da1ce-6865-4da4-8aa9-547fb69a78f7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "d69da1ce-6865-4da4-8aa9-547fb69a78f7",
    "outputId": "732c64b5-b71a-4a50-974a-3798f8a1426e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2672' max='2672' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2672/2672 09:53, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.604000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.758200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>5.288200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>5.027800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.874800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 32s, sys: 2.49 s, total: 9min 35s\n",
      "Wall time: 9min 58s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2672, training_loss=5.466920978294875, metrics={'train_runtime': 598.2251, 'train_samples_per_second': 285.785, 'train_steps_per_second': 4.467, 'total_flos': 873620128952064.0, 'train_loss': 5.466920978294875, 'epoch': 1.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f93bfea-9222-4b56-a825-3e5d3f090f5c",
   "metadata": {
    "id": "2f93bfea-9222-4b56-a825-3e5d3f090f5c"
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"./KantaiBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4face9a0-181e-439f-9cc4-b1598ce17641",
   "metadata": {
    "id": "4face9a0-181e-439f-9cc4-b1598ce17641"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"./KantaiBERT\",\n",
    "    tokenizer=\"./KantaiBERT\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a61097d5-643e-4ede-9cc5-177b71b96c00",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a61097d5-643e-4ede-9cc5-177b71b96c00",
    "outputId": "45595323-5dda-44fe-a9fc-801a5ac74806"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.04962627589702606,\n",
       "  'token': 394,\n",
       "  'token_str': ' reason',\n",
       "  'sequence': 'Human thinking involves human reason.'},\n",
       " {'score': 0.019373778253793716,\n",
       "  'token': 535,\n",
       "  'token_str': ' experience',\n",
       "  'sequence': 'Human thinking involves human experience.'},\n",
       " {'score': 0.011976179666817188,\n",
       "  'token': 610,\n",
       "  'token_str': ' conceptions',\n",
       "  'sequence': 'Human thinking involves human conceptions.'},\n",
       " {'score': 0.011964903213083744,\n",
       "  'token': 584,\n",
       "  'token_str': ' intuition',\n",
       "  'sequence': 'Human thinking involves human intuition.'},\n",
       " {'score': 0.011422745883464813,\n",
       "  'token': 616,\n",
       "  'token_str': ' cognition',\n",
       "  'sequence': 'Human thinking involves human cognition.'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"Human thinking involves human <mask>.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "A8rgbAFGG_sM",
   "metadata": {
    "id": "A8rgbAFGG_sM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
